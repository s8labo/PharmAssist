<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Talking Head with Azure TTS</title>
  <style>
    body, html { 
      width: 100%; 
      height: 100%; 
      max-width: 800px; 
      margin: auto; 
      position: relative; 
      background-color: dimgray; 
      color: white;
    }
    #avatar { 
      display: block; 
      width: 100%; 
      height: 100%;
    }
    #controls { 
      display: block; 
      position: absolute; 
      top: 10px; 
      left: 10px; 
      right: 10px; 
      height: 50px;
    }
    #text { 
      position: absolute; 
      width: Calc(100% - 110px); 
      height: 100%; 
      top: 0; 
      left: 0; 
      bottom: 0; 
      right: 110px; 
      font-family: Arial; 
      font-size: 20px; 
    }
    #speak { 
      position: absolute; 
      top: 0; 
      bottom: 0; 
      right: 0; 
      height: 100%; 
      width: 100px; 
      font-family: Arial; 
      font-size: 20px; 
    }
    #loading { 
      display: block; 
      position: absolute; 
      bottom: 10px; 
      left: 10px; 
      right: 10px; 
      height: 50px; 
      font-family: Arial; 
      font-size: 20px; 
    }
  </style>
  <!-- Azure Speech SDK for Browser -->
  <script src="https://cdn.jsdelivr.net/npm/microsoft-cognitiveservices-speech-sdk@1.15.0/distrib/browser/microsoft.cognitiveservices.speech.sdk.bundle-min.js"></script>
</head>
<body>
  <div id="avatar"></div>
  <div id="controls">
    <input id="text" type="text" value="Hallo! Wie geht es dir?">
    <button id="speak">Sprechen</button>
  </div>
  <div id="loading"></div>

  <script type="module">
    // Replace with your Azure Speech service credentials
    const AZURE_KEY = "724b12cefed2442c9df696992b249f89";  // Replace with your Azure Speech API key
    const AZURE_REGION = "https://germanywestcentral.tts.speech.microsoft.com/cognitiveservices/v1";  // Replace with your Azure region

    let synthesizer;

    document.addEventListener('DOMContentLoaded', async function () {
      const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(AZURE_KEY, AZURE_REGION);
      const audioConfig = SpeechSDK.AudioConfig.fromDefaultSpeakerOutput();

      synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, audioConfig);

      const nodeSpeak = document.getElementById('speak');
      nodeSpeak.addEventListener('click', async function () {
        const text = document.getElementById('text').value;
        if (text) {
          speakTextWithLipSync(text);
        }
      });
    });

    function speakTextWithLipSync(text) {
      const ssml = `
        <speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="http://www.w3.org/2001/mstts" xml:lang="de-DE">
          <voice name="de-DE-KatjaNeural">
            <mstts:viseme type="FacialExpression"/>
            ${text}
          </voice>
        </speak>`;

      synthesizer.speakSsmlAsync(
        ssml,
        result => {
          console.log('Speech synthesized:', result);
          // Add your lip-sync logic here based on viseme events
        },
        error => {
          console.error('Error synthesizing speech:', error);
        }
      );
    }
  </script>
</body>
</html>
